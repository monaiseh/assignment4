{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Assignment4","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"NZAN7lLKIgSp"},"source":["# Deep Neural Networks\n","\n","Welcome to the fourth lab exercise! In this exercise, you will make a L layer neural network for binary classification to classify cat and not cat images. You will implement all the building blocks of a neural network and use these building blocks to build a deep neural network with as many layers as you want. In the second lab exercise, you used logistic regression to build cat vs. non-cat images and got a 68% accuracy. The neural network algorithm will now give you an 80% accuracy!\n","\n","By completing this assignment you will:\n","\n","- Develop an intuition of the over all structure of a deep neural network.\n","\n","- Write functions (e.g. forward propagation, backward propagation, logistic loss, etc...) that would help you decompose your code and ease the process of building a deep neural network.\n","\n","- Initialize/update parameters according to your desired structure.\n","\n","**You will learn to:**\n","- This assignment will help you in building the general architecture of your deep neural network with n hidden layer and steps include:\n","    - Initializing parameters\n","    - Forward Propagation\n","    - Backward Propagation\n","    - Gradient descent \n","- Collect all the above functions into a single function in the correct order and this function will act as a main model.\n","- you will use these functions to build a deep neural network for binary image classification.\n","\n","**After this assignment you will be able to:**\n","- Use different activation functions like ReLU to improve your model\n","- Build a deeper neural network (with more than 1 hidden layer)\n","- Implement an easy-to-use neural network \n","\n","### Recommended Reading\n","\n","-   [Chapter 3](https://livebook.manning.com/book/deep-learning-with-python/chapter-3) of \"Deep Learning with Python\" by F. Chollet. \n","- [Understanding Neural Networks](https://towardsdatascience.com/understanding-neural-networks-19020b758230)\n","- [Cost function](https://towardsdatascience.com/step-by-step-the-math-behind-neural-networks-490dc1f3cfd9) of Neural Networks\n","- [Chapter 5](https://srdas.github.io/DLBook/NNDeepLearning.html) of \"Deep Learning\" \n","- [Neural Networks Part 1](https://cs231n.github.io/neural-networks-1/): Setting up the architecture.\n","- [Backward Propagation](http://neuralnetworksanddeeplearning.com/chap2.html) of \"Neural Networks and Deep Learning\".\n","- Artificial neural networks [explained](http://scs.ryerson.ca/~aharley/neural-networks/)\n","- how ANN can [represent complex functions](http://neuralnetworksanddeeplearning.com/chap4.html) \n","\n","#### Derivatives and Gradients\n","\n","- https://www.mathsisfun.com/calculus/derivatives-introduction.html\n","- (chapters 3-4) https://openstax.org/books/calculus-volume-1/pages/3-1-defining-the-derivative#27277\n","\n","**efficiently computing gradients via backpropagation:**\n","- intro level [3blue1brown educational series](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n","- more in-depth explanation [cs231n Stanford course](https://www.youtube.com/watch?v=d14TUNcbn1k)\n","\n","#### Bias and Variance trade-off\n","- [Understanding bias and variance](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n","- [Bias and Variance examples](https://towardsdatascience.com/examples-of-bias-variance-tradeoff-in-deep-learning-6420476a20bd)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yxEpRMIsOsVQ"},"source":["Let's get started\n","\n","\n","**First of all, mount google drive**\n","\n","This will mount the google drive for google colab and you will be able access contents of your drive."]},{"cell_type":"code","metadata":{"id":"d_8bjcYeIjWO"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Deep_learning_unit/Assignment4')\n","folder = os.path.join('/content/drive/My Drive/Deep_learning_unit/Assignment4')\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6yWQQgQ6QSbn"},"source":["## 1 - Packages\n","\n","First, let's run the cell below to import all the packages that you will need during this exercise.\n","- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n","- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n","- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n","- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n","- dnn_utils and dnn_app_utils provides some necessary functions for this notebook.\n","- testCases provides some test cases to assess the correctness of your functions.\n","\n","np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed. "]},{"cell_type":"code","metadata":{"id":"JFTl1A5eIgSu"},"source":["import time\n","import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","import scipy\n","from PIL import Image as Im\n","from scipy import ndimage\n","from testCases_v4a import *\n","from dnn_app import *\n","from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","np.random.seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"em6E5wGGRllo"},"source":["## 2 - Dataset\n","\n","In this lab exercise, you will use the same \"Cat vs non-Cat\" dataset as in \"Logistic Regression\" (Assignment 2). The model you had built had 68% test accuracy on classifying cats vs non-cats images. Hopefully, the new model which you will design performs better!\n","\n","You are given a dataset (\"data.h5\") containing:\n","    - a training set (\"train_catvnoncat.h5\") of m_train images labeled as cat (y=1) or non-cat (y=0) \n","    - a test set (\"test_catvnoncat.h5\") of m_test images labeled as cat or non-cat\n","    - each image is colored image of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB) and num_px are height and width of image. Thus, each image is square (height = num_px) and (width = num_px).\n","\n","\n","Let's get more familiar with the dataset. Load the data by running the cell below and check the example of an image in the dataset. Try changing the index and re-run the cell to see other images."]},{"cell_type":"code","metadata":{"id":"BsYV2LcjSJju"},"source":["train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n","\n","# Example of a picture\n","index = 50\n","plt.imshow(train_x_orig[index])\n","print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blEnS3efS6M8"},"source":["**Preprocessing your dataset**\n","\n","Try to get more familiar with the dataset. \n","\n","*   *train_set_x_orig* gives all the training images before preprocessing\n","*   *train_set_y* gives all the training image labels (1 for cat and 0 for non cat)\n","*   *test_set_x_orig* gives all the testing images before preprocessing\n","*   *test_set_y* gives all the testing image labels.\n","\n","Common steps for pre-processing a new dataset are:\n","- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n","- Reshape the datasets such that each example is now a vector of size (num_px \\* num_px \\* 3, 1)\n","- \"Standardize\" the data\n","\n","After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n","\n","**Find the values for:**\n","\n","- m_train (number of training examples)\n","- m_test (number of test examples)\n","- num_px (= height = width of a training image)\n","\n","Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). Similarly, `test_set_x_orig` is a numpy-array of shape (m_test, num_px, num_px, 3).\n","\n","Again, `train_set_y` is a numpy-array of shape (1, m_train). Similarly, `test_set_y` is a numpy-array of shape (1, m_test).\n","\n","\n","For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]\n","\n","Run the following code."]},{"cell_type":"code","metadata":{"id":"ZhPBf7NoS4br"},"source":["#Explore your dataset\n","\n","### START CODE HERE ### (≈ 3 lines of code)\n","m_train = \n","num_px = \n","m_test = \n","### END CODE HERE ###\n","\n","\n","print (\"Number of training examples: \" + str(m_train))\n","print (\"Number of testing examples: \" + str(m_test))\n","print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n","print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n","print (\"train_y shape: \" + str(train_y.shape))\n","print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n","print (\"test_y shape: \" + str(test_y.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s4KWbHBK--4a"},"source":["**Expected Output for m_train, m_test and num_px**: \n","<table style=\"width:15%\">\n","  <tr>\n","    <td>**m_train**</td>\n","    <td> 209 </td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**m_test**</td>\n","    <td> 50 </td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**num_px**</td>\n","    <td> 64 </td> \n","  </tr>\n","  \n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"6tANjAcXT9IA"},"source":["**Flattening and standardize the images**\n","\n","Flattening helps in converting the image array to a single image vector. You should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns. \n","\n","To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n","\n","But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n","\n","A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n","```python\n","X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n","```"]},{"cell_type":"code","metadata":{"id":"vjNC0k6CUVef"},"source":["\n","### START CODE HERE ### (≈ 2 lines of code)\n","train_x_flatten =                               # The \"-1\" makes reshape flatten the remaining dimensions\n","test_x_flatten = \n","### END CODE HERE ###\n","\n","# Standardize data to have feature values between 0 and 1.\n","train_x = train_x_flatten/255.\n","test_x = test_x_flatten/255.\n","\n","print (\"train_x's shape: \" + str(train_x.shape))\n","print (\"test_x's shape: \" + str(test_x.shape))\n","print (\"train_y's shape: \" + str(train_y.shape))\n","print (\"test_y's shape: \" + str(test_y.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DRwdxoTf_JFr"},"source":["**Expected Output**: \n","\n","<table style=\"width:35%\">\n","  <tr>\n","    <td>**train_x_'s shape**</td>\n","    <td> (12288, 209)</td> \n","  </tr>\n","  <tr>\n","    <td>**test_x's shape**</td>\n","    <td>(12288, 50)</td> \n","  </tr>\n","  <tr>\n","    <td>**train_y's shape**</td>\n","    <td>(1, 209)</td> \n","  </tr>\n","  <tr>\n","    <td>**test_y's shape**</td>\n","    <td>(1, 50)</td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"6i9KAJg9IgS3"},"source":["## 3 - Deep Neural Network Architecture\n","\n","To build your neural network, you will be implementing all the functions required to build an L-layer neural network. Here is an outline of the neural network architecture you will be designing:\n","\n","- Initialize the parameters for an $L$-layer neural network.\n","- Implement the forward propagation module.\n","     - Complete the LINEAR part of a layer's forward propagation step (resulting in $Z^{[l]}$).\n","     - Complete the ACTIVATION function (relu/sigmoid).\n","     - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.\n","     - Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$). This gives you a new L_model_forward function.\n","- Compute the loss.\n","- Implement the backward propagation module.\n","    - Complete the LINEAR part of a layer's backward propagation step.\n","    - Complete the gradient of the ACTIVATE function (relu_backward/sigmoid_backward) \n","    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function.\n","    - Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new L_model_backward function\n","- Finally update the parameters.\n","\n","**Notations**:\n","- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n","    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n","- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n","    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n","- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n","    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n","\n","\n","**Note** that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module you will then use the cache to calculate the gradients. \n","\n","The following figure shows the general architecture of the deep neural network to be implemented."]},{"cell_type":"code","metadata":{"id":"JhPwkVh9tikZ"},"source":["from IPython.display import Image \n","Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment4/images/final_outline.png','rb').read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilPEPF9sIgS4"},"source":["## 4 - Initialization\n","\n","The initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the `initialize_parameters_deep`, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units in layer $l$. Thus for example if the size of our input $X$ is $(12288, 209)$ (with $m=209$ examples) then:\n","\n","<table style=\"width:100%\">\n","    <tr>\n","        <td>  </td> \n","        <td> **Shape of W** </td> \n","        <td> **Shape of b**  </td> \n","        <td> **Calculation of Z** </td>\n","        <td> **Shape of activation and Z** </td> \n","    </tr>    \n","    <tr>\n","        <td> **Layer 1** </td> \n","        <td> $(n^{[1]},12288)$ </td> \n","        <td> $(n^{[1]},1)$ </td> \n","        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td>      \n","        <td> $(n^{[1]},209)$ </td> \n","    </tr>  \n","    <tr>\n","        <td> **Layer 2** </td> \n","        <td> $(n^{[2]}, n^{[1]})$  </td> \n","        <td> $(n^{[2]},1)$ </td> \n","        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n","        <td> $(n^{[2]}, 209)$ </td> \n","    </tr>\n","    <tr>\n","        <td> $\\vdots$ </td> \n","        <td> $\\vdots$  </td> \n","        <td> $\\vdots$  </td> \n","        <td> $\\vdots$</td> \n","        <td> $\\vdots$  </td> \n","   </tr>  \n","   <tr>\n","        <td> **Layer L-1** </td> \n","        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n","        <td> $(n^{[L-1]}, 1)$  </td> \n","        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n","        <td> $(n^{[L-1]}, 209)$ </td> \n","   </tr>    \n","   <tr>\n","        <td> **Layer L** </td> \n","        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n","        <td> $(n^{[L]}, 1)$ </td>\n","        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n","        <td> $(n^{[L]}, 209)$  </td> \n","    </tr>\n","</table>\n","\n","\n","**Instructions**:\n","- The model's structure is *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., it has $L-1$ layers using a ReLU activation function followed by an output layer with a sigmoid activation function.\n","- Use random initialization for the weight matrices. Use `np.random.randn(shape) / np.sqrt(layer_dims[L-1])`.\n","- Use zeros initialization for the biases. Use `np.zeros(shape)`.\n","\n","**Example:** The variable `layer_dims` stores $n^{[l]}$, the number of units in different layers. For example, [2,4,1] means two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. This means `W1`'s shape was (4,2), `b1` was (4,1), `W2` was (1,4) and `b2` was (1,1). Now you will generalize this to $L$ layers! \n","\n","\n","Here is the implementation for $L=1$ (one layer neural network). It should inspire you to implement the general case (L-layer neural network).\n","```python\n","    for Lth layer:\n","        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[L], layer_dims[L-1]) / np.sqrt(layer_dims[L-1]) \n","        parameters[\"b\" + str(L)] = np.zeros((layer_dims[L], 1))\n","```\n","\n","**Implement initialization for an L-layer Neural Network in the code below.**\n"]},{"cell_type":"code","metadata":{"id":"bDTrPyx_IgTD"},"source":["def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    parameters = {}\n","    L = len(layer_dims)            # number of layers in the network\n","\n","    for l in range(1, L):\n","      ### START CODE HERE ### (≈ 2 lines of code)\n","        parameters['W' + str(l)] =\n","        parameters['b' + str(l)] = \n","        ### END CODE HERE ###\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjIYaygeIgTG"},"source":["parameters = initialize_parameters_deep([5,4,3])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJcbrahHIgTJ"},"source":["**Expected output**:\n","       \n","<table style=\"width:80%\">\n","  <tr>\n","    <td> **W1** </td>\n","    <td>[[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n"," [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n"," [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n"," [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]</td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**b1** </td>\n","    <td>[[ 0.]\n"," [ 0.]\n"," [ 0.]\n"," [ 0.]]</td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**W2** </td>\n","    <td>[[-0.55030959  0.57236185  0.45079536  0.25124717]\n"," [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n"," [-0.13394404  0.26517773 -0.34583038 -0.19837676]]</td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**b2** </td>\n","    <td>[[ 0.]\n"," [ 0.]\n"," [ 0.]]</td> \n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"DIz8VDgV0rCL"},"source":["## 5 - Forward propagation module\n","\n","Now that you have initialized your parameters, you will do the forward propagation module. It consists of three steps where you will be implementing the following 3 functions: \n","\n"," - LINEAR Forward module\n"," - LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. \n"," - [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n","\n","Each ANN is a network of interconnected elementary computational units. These computational units are referred to as neurons. The simplest forward propagation of single neuron looks like this where it contains 2 functions, calculation of $Z$ and $A$:"]},{"cell_type":"code","metadata":{"id":"sQIlmO2K4HFP"},"source":["from IPython.display import Image \n","Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment4/images/perceptron.png','rb').read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"haD-D1Pk43Zg"},"source":["Remember that when we compute $W X + b$ in python, it carries out broadcasting. For example, if: \n","\n","$$ W = \\begin{bmatrix}\n","    j  & k  & l\\\\\n","    m  & n & o \\\\\n","    p  & q & r \n","\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n","    a  & b  & c\\\\\n","    d  & e & f \\\\\n","    g  & h & i \n","\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n","    s  \\\\\n","    t  \\\\\n","    u\n","\\end{bmatrix}\\tag{2}$$\n","\n","Then $WX + b$ will be:\n","\n","$$ WX + b = \\begin{bmatrix}\n","    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n","    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n","    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n","\\end{bmatrix}\\tag{3}  $$"]},{"cell_type":"markdown","metadata":{"id":"c8RnjwnaIgTJ"},"source":["\n","\n","### 5.1 - Linear Forward \n","\n","The linear forward module (vectorized over all the examples) computes the following equations:\n","\n","$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n","\n","where $A^{[0]} = X$. \n","\n","Build the linear part of forward propagation by calculating Z.\n","\n","**Note**: You may find `np.dot()` useful. If your dimensions don't match, printing `W.shape` may help."]},{"cell_type":"code","metadata":{"id":"FkEgBL1ZIgTK"},"source":["def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","   \n","    ### END CODE HERE ###\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lWPxNqfZIgTN"},"source":["A, W, b = linear_forward_test_case()\n","\n","Z, linear_cache = linear_forward(A, W, b)\n","print(\"Z = \" + str(Z))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wiy1q_SIgTQ"},"source":["**Expected output**:\n","\n","<table style=\"width:35%\">\n","  \n","  <tr>\n","    <td> **Z** </td>\n","    <td> [[ 3.26295337 -1.23429987]] </td> \n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Ngxo7yZgIgTQ"},"source":["### 5.2 - Linear-Activation Forward\n","\n","In this notebook, you will use two activation functions:\n","\n","- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-Z}}$ where $Z = (W A + b)$.`sigmoid` function is already built and you can directly use it. This function returns **two** items: the activation value \"`A`\" and  \"`cache`\" that contains \"`Z`\". We are storing the values in cache for the backward propagation. To use it you could just call: \n","``` python\n","A, activation_cache = sigmoid(Z)\n","```\n","\n","- **ReLU**: The mathematical formula for ReLu is $A = RELU(Z) = max(0, Z)$.  `relu` function is already built and you can directly use it. This function returns **two** items: the activation value \"`A`\" and \"`cache`\" that contains \"`Z`\". We are storing the values in cache for the backward propagation. To use it you could just call: \n","``` python\n","A, activation_cache = relu(Z)\n","```\n","\n","**Note** The sigmoid function is a popular choice for the activation function of the neurons in the final or output layer. Indeed, the value range of the sigmoid function is the interval of real number between $0$ and $1$. Thus, we can interpret the output of the sigmoid activation function as a probability or degree of belonging to a certain class or category. For the neurons in the hidden layers, the ReLU activation function is a popular choice. The figure below shows the ReLU and sigmoid functions"]},{"cell_type":"code","metadata":{"id":"fLW-GdVP6UD3"},"source":["from IPython.display import Image \n","Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment4/images/activation.png','rb').read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxg7w6z8IgTQ"},"source":["**Implement the forward propagation of the *LINEAR->ACTIVATION* layer.** \n","\n","Mathematical relation is: $A^{[l]} = g(Z^{[l]})$  and  $Z = W^{[l]}A^{[l-1]} +b^{[l]}$ where the activation \"g\" can be sigmoid() or relu(). Use linear_forward() and the correct activation function.\n","\n","Basically, you are going to group two functions (Linear and Activation) into one function (LINEAR->ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.\n","\n"]},{"cell_type":"code","metadata":{"id":"9mTzYnUaIgTR"},"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","          ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = \n","        A, activation_cache = \n","        ### END CODE HERE ###\n","    \n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","          ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = \n","        A, activation_cache =\n","        ### END CODE HERE ###\n","    \n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VLBn6_q5IgTS"},"source":["A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(A))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ctLvAcwIgTW"},"source":["**Expected output**:\n","       \n","<table style=\"width:35%\">\n","  <tr>\n","    <td> **With sigmoid: A ** </td>\n","    <td > [[ 0.96890023  0.11013289]]</td> \n","  </tr>\n","  <tr>\n","    <td> **With ReLU: A ** </td>\n","    <td > [[ 3.43896131  0.        ]]</td> \n","  </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"--8el5MWIgTW"},"source":["**Note**: In deep learning, the \"[LINEAR->ACTIVATION]\" computation is counted as a single layer in the neural network, not two layers. "]},{"cell_type":"markdown","metadata":{"id":"6KqOsQguIgTW"},"source":["### 5.3 Forward Propagation of L-Layer Model \n","\n","For even more convenience when implementing the $L$-layer Neural Net, you will need a function that replicates the previous one (`linear_activation_forward` with RELU) $L-1$ times, then follows that with one `linear_activation_forward` with SIGMOID.\n","\n"," "]},{"cell_type":"code","metadata":{"id":"7__YcbmMAXr8"},"source":["from IPython.display import Image \n","Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment4/images/model_architecture_kiank.png','rb').read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxpdhoAIAn22"},"source":["Implement the forward propagation of the above model.\n","\n","**Instruction**: In the code below, the variable `AL` will denote $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is sometimes also called `Yhat`, i.e., this is $\\hat{Y}$.) \n","For all other layers except last layer, you will use ReLU as activation function.\n","\n","**Tips**:\n","- Use the functions you had previously written \n","- Use a for loop to replicate [LINEAR->RELU] (L-1) times\n","- Don't forget to keep track of the caches in the \"caches\" list. To add a new value `c` to a `list`, you can use `list.append(c)`."]},{"cell_type":"code","metadata":{"id":"GkS8fX7HIgTX"},"source":["def L_model_forward(X, parameters):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","    \n","    Returns:\n","    AL -- last post-activation value\n","    caches -- list of caches containing:\n","                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n","                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2                  # number of layers in the neural network\n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","       ### START CODE HERE ### (≈ 2 lines of code)\n","        A, cache = \n","        \n","        ### END CODE HERE ###\n","    \n","\n","   # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","   ### START CODE HERE ### (≈ 2 lines of code)\n","    AL, cache = \n","    \n","    ### END CODE HERE ###\n","    \n","    assert(AL.shape == (1,X.shape[1]))\n","            \n","    return AL, caches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YzB_W9zbIgTa"},"source":["X, parameters = L_model_forward_test_case_2hidden()\n","AL, caches = L_model_forward(X, parameters)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A31M5MYNIgTd"},"source":["<table style=\"width:50%\">\n","  <tr>\n","    <td> **AL** </td>\n","    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n","  </tr>\n","  <tr>\n","    <td> **Length of caches list ** </td>\n","    <td > 3 </td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"_XA2Lr3MIgTd"},"source":["Great! Now you have a full forward propagation that takes the input X and outputs a row vector $A^{[L]}$ containing your predictions. It also records all intermediate values in \"caches\". Using $A^{[L]}$, you can compute the cost of your predictions."]},{"cell_type":"markdown","metadata":{"id":"2YX2x7J-IgTe"},"source":["## 6 - Cost function\n","\n","In this section, you will compute the cost, because you want to check if your model is actually learning.\n","\n","**Exercise**: Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"]},{"cell_type":"code","metadata":{"id":"PLJ1oIQXIgTe"},"source":["def compute_cost(AL, Y):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","\n","    # Compute cost from AL and Y.\n","    ### START CODE HERE ### (≈ 1 lines of code)\n","   \n","    ### END CODE HERE ###\n","    \n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"laupYb6HIgTf"},"source":["Y, AL = compute_cost_test_case()\n","\n","print(\"cost = \" + str(compute_cost(AL, Y)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txDgtpHSIgTi"},"source":["**Expected Output**:\n","\n","<table>\n","    <tr>\n","    <td>**cost** </td>\n","    <td> 0.2797765635793422</td> \n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"aCCoyzYPIgTi"},"source":["## 7 - Backward propagation module\n","\n","Just like with forward propagation, you will implement three functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters. \n","\n","For those of you who are interested in calculus, the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ i.e. $\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}}$ (represented as $dz^{[1]}$) in a 2-layer network as follows:\n","\n","$$dz^{[1]} = \\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n","\n","In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you can do \n","\n","$$dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$$\n","\n","During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n","\n","Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you can do \n","\n","$$db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$$\n","\n","This is why we say it **backpropagation**.\n","\n","\n","Now, similar to forward propagation, you are going to build the backward propagation in three steps:\n","- LINEAR backward\n","- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n","- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"]},{"cell_type":"code","metadata":{"id":"srgyjqatIk1F"},"source":["from IPython.display import Image \n","Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment4/images/backprop_kiank.png','rb').read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1TQ7ndivIgTj"},"source":["### 7.1 - Linear backward\n","\n","For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n","\n","Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"e6RAeuc5JFiB"},"source":["from IPython.display import Image \n","Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment4/images/linearback_kiank.png','rb').read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWcrCVI9IgTk"},"source":["The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n","$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n","$$ db^{[l]} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n","$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n","\n","\n","\n","**Exercise** Use the above 3 formulas above to implement linear_backward()."]},{"cell_type":"code","metadata":{"id":"dK07D3ovIgTm"},"source":["def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    # Compute dW, db and dA_prev\n","    ### START CODE HERE ### (≈ 3 lines of code).  \n","\n","\n","\n","    ### END CODE HERE ###\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGn7HHE2IgTo"},"source":["# Set up some test inputs\n","dZ, linear_cache = linear_backward_test_case()\n","\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1oNVDTtIgTp"},"source":["** Expected Output**:\n","    \n","```\n","dA_prev = \n"," [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW = \n"," [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db = \n"," [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"63PiL86gIgTq"},"source":["### 7.2 - Linear-Activation backward\n","\n","Next, you will create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n","\n","To help you implement `linear_activation_backward`, we provided two backward functions:\n","- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n","\n","```python\n","dZ = sigmoid_backward(dA, activation_cache)\n","```\n","\n","- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n","\n","```python\n","dZ = relu_backward(dA, activation_cache)\n","```\n","\n","If $g(.)$ is the activation function, \n","`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n","\n","**Exercise**: Implement the backpropagation for the *LINEAR->ACTIVATION* layer."]},{"cell_type":"code","metadata":{"id":"KZxjaPXUIgTq"},"source":["def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","    \n","    Arguments:\n","    dA -- post-activation gradient for current layer l \n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","    \n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        dZ = \n","        dA_prev, dW, db = \n","        ### END CODE HERE ###\n","        \n","    elif activation == \"sigmoid\":\n","      ### START CODE HERE ### (≈ 2 lines of code)\n","        dZ = \n","        dA_prev, dW, db = \n","        ### END CODE HERE ###\n","    \n","    return dA_prev, dW, db\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQRTdD6cIgTt"},"source":["dAL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjo794jQIgTw"},"source":["**Expected output with sigmoid:**\n","\n","<table style=\"width:100%\">\n","  <tr>\n","    <td > dA_prev </td> \n","           <td >[[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]] </td> \n","  </tr>  \n","    <tr>\n","    <td > dW </td> \n","           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n","  </tr>  \n","    <tr>\n","    <td > db </td> \n","           <td > [[-0.05729622]] </td> \n","  </tr> \n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A5lILywCIgTx"},"source":["**Expected output with relu:**\n","\n","<table style=\"width:100%\">\n","  <tr>\n","    <td > dA_prev </td> \n","           <td > [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]] </td> \n","  </tr>   \n","    <tr>\n","    <td > dW </td> \n","           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n","  </tr>   \n","    <tr>\n","    <td > db </td> \n","           <td > [[-0.20837892]] </td> \n","  </tr> \n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UVpYQ2wjIgTx"},"source":["### 7.3 - L-Model Backward \n","\n","Now you will implement the backward function for the whole network. Recall that when you implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the `L_model_backward` function, you will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$. Figure below shows the backward pass. \n","\n"]},{"cell_type":"code","metadata":{"id":"ChY5dla_LQ96"},"source":["from IPython.display import Image \n","Image(open('/content/drive/My Drive/Deep_learning_unit/Assignment4/images/mn_backward.png','rb').read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4yCCSIPgLx4E"},"source":["**Initializing backpropagation**:\n","To backpropagate through this network, we know that the output is, \n","$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL`.\n","To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n","```python\n","dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n","```\n","\n","You can then use this post-activation gradient `dAL` to keep going backward. As seen in Figure 5, you can now feed in `dAL` into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula : \n","\n","$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n","\n","For example, for $l=3$ this would store $dW^{[l]}$ in `grads[\"dW3\"]`.\n","\n","**Exercise**: Implement backpropagation for the *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model."]},{"cell_type":"code","metadata":{"id":"WWcQrw3-IgT0"},"source":["def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","    \n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n","    \n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","    \n","    # Initializing the backpropagation, calculate dAL\n","    ### START CODE HERE ### (1 line of code)\n","   \n","    ### END CODE HERE ###\n","    \n","\n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","    ### START CODE HERE ### (approx. 2 lines)\n","    current_cache = \n","    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \n","    ### END CODE HERE ###\n","    \n","       # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n","        ### START CODE HERE ### (approx. 5 lines)\n","        current_cache = \n","        dA_prev_temp, dW_temp, db_temp = \n","        grads[\"dA\" + str(l)] = \n","        grads[\"dW\" + str(l + 1)] =\n","        grads[\"db\" + str(l + 1)] = \n","        ### END CODE HERE ###\n","        \n","\n","    return grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tulvaA-4IgT3"},"source":["AL, Y_assess, caches = L_model_backward_test_case()\n","grads = L_model_backward(AL, Y_assess, caches)\n","print_grads(grads)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFZYwKsoIgT4"},"source":["**Expected Output**\n","\n","<table style=\"width:60%\">\n","  <tr>\n","    <td > dW1 </td> \n","           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n"," [ 0.          0.          0.          0.        ]\n"," [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n","  </tr>   \n","    <tr>\n","    <td > db1 </td> \n","           <td > [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]] </td> \n","  </tr>  \n","  <tr>\n","  <td > dA1 </td> \n","           <td > [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]] </td> \n","  </tr> \n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HD-12LooIgT5"},"source":["## 8 - Update Parameters\n","\n","In this section you will update the parameters of the model, using gradient descent: \n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n","\n","where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "]},{"cell_type":"markdown","metadata":{"id":"wNddAkaQIgT5"},"source":["Implement `update_parameters()` to update your parameters using gradient descent.\n","\n","\n","Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$. \n"]},{"cell_type":"code","metadata":{"id":"x4BS48UWIgT5"},"source":["def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter . Use a for loop.\n","      ### START CODE HERE ### (≈ 3 lines of code)\n","   \n","\n","\n","    ### END CODE HERE ###\n","        \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9CVIAJGIgT6"},"source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parameters[\"W1\"]))\n","print (\"b1 = \"+ str(parameters[\"b1\"]))\n","print (\"W2 = \"+ str(parameters[\"W2\"]))\n","print (\"b2 = \"+ str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXlhG_IlDQeg"},"source":["**Expected Output**:\n","\n","<table style=\"width:100%\"> \n","    <tr>\n","    <td > W1 </td> \n","           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n","  </tr> \n","    <tr>\n","    <td > b1 </td> \n","           <td > [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]] </td> \n","  </tr> \n","  <tr>\n","    <td > W2 </td> \n","           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n","  </tr>  \n","    <tr>\n","    <td > b2 </td> \n","           <td > [[-0.84610769]] </td> \n","  </tr> \n","</table>"]},{"cell_type":"code","metadata":{"id":"HyS2J3l_c1Br"},"source":["def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  L-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    n = len(parameters) // 2 # number of layers in the neural network\n","    p = np.zeros((1,m))\n","    \n","    # Forward propagation\n","    ### START CODE HERE ### (≈ 1 lines of code)\n","   probas, caches = \n","    ### END CODE HERE ###\n","\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, probas.shape[1]):\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","       \n","\n","        ### END CODE HERE ###\n","    \n","    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n","        \n","    return p"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jKO3V4-hG0-"},"source":["## 9 - Create L-layer Neural Network Model\n","\n"," Use the helper functions you have implemented previously to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. The functions you may need and their inputs are:\n","```python\n","def initialize_parameters_deep(layers_dims):\n","    ...\n","    return parameters \n","def L_model_forward(X, parameters):\n","    ...\n","    return AL, caches\n","def compute_cost(AL, Y):\n","    ...\n","    return cost\n","def L_model_backward(AL, Y, caches):\n","    ...\n","    return grads\n","def update_parameters(parameters, grads, learning_rate):\n","    ...\n","    return parameters\n","```"]},{"cell_type":"code","metadata":{"id":"RWs8CLuUvIL9"},"source":["### CONSTANTS ###\n","layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TsR9VFppvMH0"},"source":["# GRADED FUNCTION: L_layer_model\n","\n","def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n","    \"\"\"\n","    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n","    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n","    learning_rate -- learning rate of the gradient descent update rule\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- if True, it prints the cost every 100 steps\n","    \n","    Returns:\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\"\n","\n","    np.random.seed(1)\n","    costs = []                         # keep track of cost\n","    \n","    # Parameters initialization. (≈ 1 line of code)\n","    ### START CODE HERE ###\n","    \n","    ### END CODE HERE ###\n","    \n","    # Loop (gradient descent)\n","    for i in range(0, num_iterations):\n","\n","        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        \n","        ### END CODE HERE ###\n","        \n","        # Compute cost.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        \n","        ### END CODE HERE ###\n","    \n","        # Backward propagation.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        \n","        ### END CODE HERE ###\n"," \n","        # Update parameters.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        \n","        ### END CODE HERE ###\n","                \n","        # Print the cost every 100 training example\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","        if print_cost and i % 100 == 0:\n","            costs.append(cost)\n","            \n","    # plot the cost\n","    plt.plot(np.squeeze(costs))\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (per hundreds)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pH0TWMN_hXUP"},"source":["You will now train the model as a 4-layer neural network.\n","Run the cell below to train your model. The cost should decrease on every iteration. It may take up to 5 minutes to run 2500 iterations. Check if the \"Cost after iteration 0\" matches the expected output below, if not click on the square (⬛) on the upper bar of the notebook to stop the cell and try to find your error."]},{"cell_type":"code","metadata":{"id":"iwDJXtl2vO5p"},"source":["parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GYnfF5tSo_Km"},"source":["**Expected Output**:\n","<table> \n","    <tr>\n","        <td> **Cost after iteration 0**</td>\n","        <td> 0.771749 </td>\n","    </tr>\n","    <tr>\n","        <td> **Cost after iteration 100**</td>\n","        <td> 0.672053 </td>\n","    </tr>\n","    <tr>\n","        <td> **...**</td>\n","        <td> ... </td>\n","    </tr>\n","    <tr>\n","        <td> **Cost after iteration 2400**</td>\n","        <td> 0.092878 </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"MeAKv7GSvRhE"},"source":["pred_train = predict(train_x, train_y, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9vcCfUA_pEgB"},"source":["<table>\n","    <tr>\n","    <td>\n","    **Train Accuracy**\n","    </td>\n","    <td>\n","    0.985645933014\n","    </td>\n","    </tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"Xuet808ppE98"},"source":["pred_test = predict(test_x, test_y, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqGakYtKpK5h"},"source":["**Expected Output**:\n","\n","<table> \n","    <tr>\n","        <td> **Test Accuracy**</td>\n","        <td> 0.8 </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"AghzRAqPpUld"},"source":["It seems that your 4-layer neural network has better performance (80%) than your logistic regression (68%) on the same test set. "]},{"cell_type":"markdown","metadata":{"id":"8MO8QVVNphDI"},"source":["##  10- Results Analysis\n","\n","First, let's take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images. "]},{"cell_type":"code","metadata":{"id":"Iugiqo-kplJc"},"source":["print_mislabeled_images(classes, test_x, test_y, pred_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CnTR-OH5pr8P"},"source":["### Test with your own image\n","\n","Try the classifier you built on your own image and check the output of the model:\n","1. Add your image in the \"images\" folder in Assignment2 folder on your google drive \n","2. Change your image's name in the following code\n","3. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"]},{"cell_type":"code","metadata":{"id":"cQwfnQltpn2B"},"source":["## START CODE HERE ##\n","my_image = \"catc.png\" # change this to the name of your image file \n","my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n","## END CODE HERE ##\n","\n","# We preprocess the image to fit your algorithm.\n","fname = \"images/\" + my_image\n","image = Im.open(fname)\n","my_image = image.resize((num_px,num_px))\n","my_image=np.array(my_image).reshape((1, num_px*num_px*3)).T\n","my_image = my_image/255.\n","my_predicted_image = predict(my_image, my_label_y, parameters)\n","\n","plt.imshow(image)\n","\n","print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyCKigNLIgT8"},"source":["\n","##Conclusion\n","\n","Congrats on implementing all the functions required for building a deep neural network! \n","\n","You have in fact used this models for calssification of cat vs non-cat images!"]}]}